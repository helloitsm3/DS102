{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i2.wp.com/hackwagon.com/wp-content/uploads/2017/02/Logo-Web-Export.png?ssl=1\" width=200/></center>\n",
    "<h1> Hackwagon Academy DS102 Lesson 4B </h1>\n",
    "<h2> Text Mining</h2> \n",
    "<h3> Lesson Outline </h3>\n",
    "\n",
    "- 1. [Text Analysis](#1)\n",
    "- 2. [Terminologies](#2)\n",
    "- 3. [Text Normalisation](#3)\n",
    "- 4. [Simple Cleaning](#4)\n",
    "    - 4.1 [Lowercase](#4.1)\n",
    "    - 4.2 [Strip Spaces](#4.2)\n",
    "    - 4.3 [Regex Cleaning](#4.3)\n",
    "    - [Practice I](#P1)\n",
    "- 5. [Tokenisation](#5)\n",
    "- 6. [Method 1 - Lemmatisation](#6)\n",
    "    - 6.1 [POS Tagging](#6.1)\n",
    "    - 6.2 [Lemmatisation](#6.2)\n",
    "- 7. [Method 2 - Stemming](#7)\n",
    "- 8. [Stemming vs. Lemming](#8)\n",
    "- 9. [Stop Word Removal](#9)\n",
    "- [Practice II](#P2)\n",
    "- 10. [Sentiment Analysis](#10)\n",
    "    - 10.1 [VADER](#10.1)\n",
    "    - 10.2 [Naive Bayes Classification](#10.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "<a id='1'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;1.</font><font color=\"salmon\"> Text Analysis </font> </h2></a>\n",
    "\n",
    "<img src=\"https://i.imgur.com/pqrH1r3.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "NLP is Natural Language Processing, finding useful insights from unstructured textual data. \n",
    "\n",
    "**Practical**\n",
    "\n",
    "* Spam Detection\n",
    "* Censorship / Filtering Sexually Explicit Content\n",
    "* Organizing / Categorizing Documents\n",
    "* Determining people's impression of your company\n",
    "\n",
    "**Theoretical**\n",
    "\n",
    "* Understanding intent of words (question answering)\n",
    "* Understanding references in sentences (coreference resolution)\n",
    "    * The man tried to pick up his son but he was weak. Who does \"he\" refer to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;2.</font><font color=\"salmon\"> Terminologies </font> </h2></a>\n",
    "\n",
    "In Text Mining, we often use the following terms to refer to collections of words\n",
    "\n",
    "- <b>Corpus</b>\n",
    "    - Large collection of texts, represented by documents\n",
    "    - Corpora is a collection of corpus\n",
    "- <b>Document</b>\n",
    "    - Contains multiple words and strung together therefore producing meaning\n",
    "- <b>Term</b>\n",
    "    - A word is a term\n",
    "\n",
    "<img src=\"https://i.imgur.com/F3fSS1v.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Corpus\n",
    "\n",
    "To do either Lemmatisation (Lemming) or Stemming, we will use the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seanl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\seanl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\seanl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\seanl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Downloading word corpus\n",
    "nltk.download('punkt') # STEMMING\n",
    "nltk.download('averaged_perceptron_tagger') # < POS TAGGING\n",
    "nltk.download('wordnet') # LEMMATISATION\n",
    "nltk.download('stopwords') # STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;3.</font><font color=\"salmon\"> Text Normalisation </font> </h2></a>\n",
    "\n",
    "The purpose of text normalisation is to find/retain the <b>root form of a word.</b> Take the following as an example: \n",
    "\n",
    "<img src=\"https://i.imgur.com/kSWH6i7.png\" width=600 />\n",
    "\n",
    "Text Normalisation involves several steps and depending on your use case, you can pick either Stemming or Lemmatization.\n",
    "\n",
    "<img src=\"https://i.imgur.com/NuOTXxL.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;4.</font><font color=\"salmon\"> Simple Cleaning </font> </h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'><h3>4.1 Lowercase</h3></a>\n",
    "\n",
    "Make all characters within a string a lower case with `.lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hackwagon'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"HACKWAGON\"\n",
    "name.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'><h3>4.2 Strip Spaces</h3></a>\n",
    "\n",
    "Remove extra spaces within the string by using the `.strip()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HACKWAGON'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"      HACKWAGON     \"\n",
    "name.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'><h3>4.3 Regex Cleaning</h3></a>\n",
    "\n",
    "Use the regular expression library to remove unwanted characters. To learn more about the regular expression library, click [here](https://regexone.com/)\n",
    "\n",
    "<img src=\"https://i.imgur.com/bJJ9MBD.png\" width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hackwagon'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "name = \"(Hackwagon)\"\n",
    "name = re.sub(\"[.®'&$’\\\"\\-()]\", \"\", name)\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='P1'><h2> <img src=\"https://cdn.shopify.com/s/files/1/1200/7374/products/book_aec28e76-52ec-44ab-bc01-41df1279c89f_550x825.png?v=1473897430\" width=25 align=\"left\"> <font color=\"darkorange\"> &nbsp; Practice I </font><font color=\"skyblue\"> * </font></h2></a>\n",
    "\n",
    "### Songs-100.csv\n",
    "\n",
    "Using the `songs-100.csv`, create a DataFrame called `songs_df` and preview the DataFrame with `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shape of You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despacito - Remix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despacito (Featuring Daddy Yankee)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Something Just Like This</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm the One</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name\n",
       "0                        Shape of You\n",
       "1                   Despacito - Remix\n",
       "2  Despacito (Featuring Daddy Yankee)\n",
       "3            Something Just Like This\n",
       "4                         I'm the One"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "songs_df = pd.read_csv(\"../Datasets/songs-100.csv\")\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do simple cleaning function \n",
    "\n",
    "Using what you've learnt earlier, create a function called `clean_names()` which will clean every song name by:\n",
    "\n",
    "* Removes characters with the pattern `[.®'&$’\\\"\\-()]`\n",
    "* Lowercases the string\n",
    "* Strips extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_names(song_title):\n",
    "    song_title = song_title.strip()\n",
    "    song_title = song_title.lower()\n",
    "    song_title = re.sub(\"[.®'&$’\\\"\\-()]\", \"\", song_title)\n",
    "    return song_title\n",
    "    # return the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cleaning function \n",
    "\n",
    "Apply the clean function on the `names` column and <b>reassign it back to the <code>names</code> column</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shape of You</td>\n",
       "      <td>shape of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despacito - Remix</td>\n",
       "      <td>despacito  remix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despacito (Featuring Daddy Yankee)</td>\n",
       "      <td>despacito featuring daddy yankee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Something Just Like This</td>\n",
       "      <td>something just like this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm the One</td>\n",
       "      <td>im the one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name                     cleaned_title\n",
       "0                        Shape of You                      shape of you\n",
       "1                   Despacito - Remix                  despacito  remix\n",
       "2  Despacito (Featuring Daddy Yankee)  despacito featuring daddy yankee\n",
       "3            Something Just Like This          something just like this\n",
       "4                         I'm the One                        im the one"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df[\"cleaned_title\"] = songs_df[\"name\"].apply(clean_names)\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> End of Practice I </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;5.</font><font color=\"salmon\"> Tokenisation </font> </h2></a>\n",
    "\n",
    "Tokenisation is the process of splitting up each word by spaces into individual tokens. \n",
    "\n",
    "<img src=\"https://i.imgur.com/LSpV9Y1.png\" width=400>\n",
    "\n",
    "To do so, we will use the `nltk` - `word-tokenize()` function to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['shape', 'of', 'you'],\n",
       " ['despacito', 'remix'],\n",
       " ['despacito', 'featuring', 'daddy', 'yankee'],\n",
       " ['something', 'just', 'like', 'this'],\n",
       " ['im', 'the', 'one'],\n",
       " ['humble'],\n",
       " ['it', 'aint', 'me', 'with', 'selena', 'gomez'],\n",
       " ['unforgettable'],\n",
       " ['thats', 'what', 'i', 'like'],\n",
       " ['i',\n",
       "  'dont',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'live',\n",
       "  'forever',\n",
       "  'fifty',\n",
       "  'shades',\n",
       "  'darker',\n",
       "  'from',\n",
       "  'fifty',\n",
       "  'shades',\n",
       "  'darker',\n",
       "  'original',\n",
       "  'motion',\n",
       "  'picture',\n",
       "  'soundtrack'],\n",
       " ['xo', 'tour', 'llif3'],\n",
       " ['paris'],\n",
       " ['stay', 'with', 'alessia', 'cara'],\n",
       " ['attention'],\n",
       " ['mask', 'off'],\n",
       " ['congratulations'],\n",
       " ['swalla', 'feat', 'nicki', 'minaj', 'ty', 'dolla', 'ign'],\n",
       " ['castle', 'on', 'the', 'hill'],\n",
       " ['rockabye', 'feat', 'sean', 'paul', 'annemarie'],\n",
       " ['believer'],\n",
       " ['mi', 'gente'],\n",
       " ['thunder'],\n",
       " ['say', 'you', 'wont', 'let', 'go'],\n",
       " ['theres', 'nothing', 'holdin', 'me', 'back'],\n",
       " ['me', 'rehúso'],\n",
       " ['issues'],\n",
       " ['galway', 'girl'],\n",
       " ['scared', 'to', 'be', 'lonely'],\n",
       " ['closer'],\n",
       " ['symphony', 'feat', 'zara', 'larsson'],\n",
       " ['i', 'feel', 'it', 'coming'],\n",
       " ['starboy'],\n",
       " ['wild', 'thoughts'],\n",
       " ['slide'],\n",
       " ['new', 'rules'],\n",
       " ['18002738255'],\n",
       " ['passionfruit'],\n",
       " ['rockstar'],\n",
       " ['strip', 'that', 'down'],\n",
       " ['2u', 'feat', 'justin', 'bieber'],\n",
       " ['perfect'],\n",
       " ['call', 'on', 'me', 'ryan', 'riback', 'extended', 'remix'],\n",
       " ['feels'],\n",
       " ['mama'],\n",
       " ['felices', 'los', '4'],\n",
       " ['ispy', 'feat', 'lil', 'yachty'],\n",
       " ['location'],\n",
       " ['chantaje'],\n",
       " ['bad', 'and', 'boujee', 'feat', 'lil', 'uzi', 'vert'],\n",
       " ['havana'],\n",
       " ['solo', 'dance'],\n",
       " ['fake', 'love'],\n",
       " ['let', 'me', 'love', 'you'],\n",
       " ['more', 'than', 'you', 'know'],\n",
       " ['one', 'dance'],\n",
       " ['subeme', 'la', 'radio'],\n",
       " ['pretty', 'girl', 'cheat', 'codes', 'x', 'cade', 'remix'],\n",
       " ['sorry', 'not', 'sorry'],\n",
       " ['redbone'],\n",
       " ['24k', 'magic'],\n",
       " ['dna'],\n",
       " ['el', 'amante'],\n",
       " ['you', 'dont', 'know', 'me', 'radio', 'edit'],\n",
       " ['chained', 'to', 'the', 'rhythm'],\n",
       " ['no', 'promises', 'feat', 'demi', 'lovato'],\n",
       " ['dont', 'wan', 'na', 'know', 'feat', 'kendrick', 'lamar'],\n",
       " ['how', 'far', 'ill', 'go', 'from', 'moana'],\n",
       " ['slow', 'hands'],\n",
       " ['escápate', 'conmigo'],\n",
       " ['bounce', 'back'],\n",
       " ['sign', 'of', 'the', 'times'],\n",
       " ['goosebumps'],\n",
       " ['young', 'dumb', 'broke'],\n",
       " ['there', 'for', 'you'],\n",
       " ['cold', 'feat', 'future'],\n",
       " ['silence'],\n",
       " ['too', 'good', 'at', 'goodbyes'],\n",
       " ['just', 'hold', 'on'],\n",
       " ['look', 'what', 'you', 'made', 'me', 'do'],\n",
       " ['glorious', 'feat', 'skylar', 'grey'],\n",
       " ['starving'],\n",
       " ['reggaetón', 'lento', 'bailemos'],\n",
       " ['weak'],\n",
       " ['side', 'to', 'side'],\n",
       " ['otra', 'vez', 'feat', 'j', 'balvin'],\n",
       " ['i', 'like', 'me', 'better'],\n",
       " ['in', 'the', 'name', 'of', 'love'],\n",
       " ['cold', 'water', 'feat', 'justin', 'bieber', 'mø'],\n",
       " ['malibu'],\n",
       " ['all', 'night'],\n",
       " ['hear', 'me', 'now'],\n",
       " ['your', 'song'],\n",
       " ['ahora', 'dice'],\n",
       " ['friends', 'with', 'bloodpop'],\n",
       " ['bank', 'account'],\n",
       " ['bad', 'things', 'with', 'camila', 'cabello'],\n",
       " ['dont', 'let', 'me', 'down'],\n",
       " ['body', 'like', 'a', 'back', 'road'],\n",
       " ['now', 'or', 'never'],\n",
       " ['dusk', 'till', 'dawn', 'radio', 'edit']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# tokenize each title using list comprehension\n",
    "song_titles = songs_df['cleaned_title'].tolist()\n",
    "tokenized = []\n",
    "\n",
    "for song in song_titles:\n",
    "    tokenized.append(word_tokenize(song))\n",
    "    \n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;6.</font><font color=\"salmon\"> Method 1 - Lemmatisation </font> </h2></a>\n",
    "\n",
    "Lemmatisation is a more accurate but slower version of stemming. It works in two parts:\n",
    "\n",
    "1. Part Of Speech Tagging\n",
    "2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'><h3>6.1 Part-Of-Speech (POS) Tagging</h3></a>\n",
    "\n",
    "POS Tagging basically classifies a word as a \n",
    "\n",
    "* nouns,\n",
    "* verbs,\n",
    "* adjectives,\n",
    "* adverbs,\n",
    "* etc.\n",
    "\n",
    "<img src=\"https://i.imgur.com/GQTUrJk.png\" width=800>\n",
    "\n",
    "It takes a list of tokens and adds a tag to each of them\n",
    "\n",
    "<img src=\"https://i.imgur.com/1hDGlsu.png\" width=\"500\"/>\n",
    "\n",
    "The meaning of these tags can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "# nltk.pos_tag(a list of tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'><h3>6.2 Lemmatisation</h3></a>\n",
    "\n",
    "<img src=\"https://i.imgur.com/dcps8NV.png\" width=\"400\"/>\n",
    "\n",
    "Lemmatisation, like stemming converts words to their root form.\n",
    "\n",
    "* **HOWEVER**, unlike stemming it doesn't arbitrarily chop off letters.\n",
    "* Instead, it refers to a vocabulary and converts words into their base form\n",
    "* The tokens need to be tagged (which you just did)\n",
    "\n",
    "```\n",
    "  Raw               Stemming        Lemmatisation\n",
    "  --------------    --------------  --------------\n",
    "  consolidating          consolid       consolidate\n",
    "```\n",
    "\n",
    "From the above example we can see that **consolidating** is converted to **consolidate**\n",
    "\n",
    "#### Lemmatisation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lemmatisation as a Function \n",
    "\n",
    "As the lemmatizer works on only a single word, you will have to apply it to each pair to get a lemmatized sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(pair):\n",
    "    word, tag = pair\n",
    "\n",
    "    try:\n",
    "        return lemmatizer.lemmatize(word, pos=tag[0].lower())\n",
    "    except KeyError:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "# nltk.pos_tag(a list of tokens)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "lemmatized = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;7.</font><font color=\"salmon\"> Method 2 - Stemming </font> </h2></a>\n",
    "\n",
    "Stemming makes words common by chopping off the ends of a word. \n",
    "\n",
    "A common algorithm is Porter's Algorithm. It will convert all of the following words to **`oper`**\n",
    "\n",
    "<img src=\"https://i.imgur.com/trzAobw.png\" width=\"200\"/>\n",
    "\n",
    "How it works is by applying a set of rules to the word\n",
    "\n",
    "**Step 1a** of the Porter Algorithm\n",
    "```\n",
    "SSES ->  SS        caresses  ->  caress\n",
    "IES  ->  I         ponies  ->  poni\n",
    "SS  ->  SS         caress  ->  caress\n",
    "S  ->              cats  ->  cat\n",
    "```\n",
    "\n",
    "More steps [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create a stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Now you can stem any word you want\n",
    "stemmer.stem(word)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem each word using list comprehension\n",
    "stemmed = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;8.</font><font color=\"salmon\"> Stemming vs. Lemming </font> </h2></a>\n",
    "\n",
    "<img src=\"https://i.imgur.com/7CgyWwH.png\" width=500>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <b>Below is a difference between Stemming and Lemmatization</b>\n",
    "\n",
    " ```\n",
    "   Raw               Stemming        Lemmatisation\n",
    "   --------------    --------------  --------------\n",
    "              I                 I                 I\n",
    "             am                am                be\n",
    "  consolidating          consolid       consolidate\n",
    "         credit            credit            credit\n",
    "           card              card              card\n",
    "           debt              debt              debt\n",
    "       incurred             incur             incur\n",
    "           over              over              over\n",
    "          three             three             three\n",
    "          years              year              year\n",
    "            ago               ago               ago\n",
    "            and               and               and\n",
    "         having              have              have\n",
    "              a                 a                 a\n",
    "       concrete           concret          concrete\n",
    "            end               end               end\n",
    "             in                in                in\n",
    "          sight             sight             sight\n",
    "             is                is                be\n",
    "           more              more              more\n",
    "     motivating             motiv        motivating\n",
    "              .                 .                 .\n",
    "              I                 I                 I\n",
    "             am                am                be\n",
    "        eagerly           eagerli           eagerly\n",
    "       striving            strive          striving\n",
    "        towards            toward           towards\n",
    "       becoming             becom            become\n",
    "     completely           complet        completely\n",
    "           debt              debt              debt\n",
    "           free              free              free\n",
    " ```\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;9.</font><font color=\"salmon\"> Stop Word Removal </font> </h2></a>\n",
    "\n",
    "Stop words are words which have little to no meaning on the overall sentence. For example, they are\n",
    "\n",
    "* i\n",
    "* me\n",
    "* my\n",
    "* or\n",
    "* a\n",
    "* an\n",
    "\n",
    "A more complete list can be downloaded from the **`stopwords` corpus**.\n",
    "\n",
    "The **`stopwords`** corpus that you downloaded is basically a list words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To remove the stop words, you just filter out tokens which are in the list of stop words\n",
    "\n",
    "* For a string `text` and a list `l`, you can check whether `l` contains `text` by doing\n",
    "\n",
    "```python\n",
    "\n",
    "    # Example\n",
    "    text = 'a'\n",
    "    ab_list = ['a','b']\n",
    "\n",
    "    text in ab_list # True\n",
    "    'c' in ab_list # False\n",
    "```\n",
    "\n",
    "* Here's how an example of how you can choose only tokens that don't exist in stopwords\n",
    "\n",
    "\n",
    "```python\n",
    "tokens = ['Hi', 'I']\n",
    "\n",
    "no_stop_words = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in STOP_WORDS:\n",
    "            no_stop_words.append(word)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens =['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "# Write code below\n",
    "filtered = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='P2'><h2> <img src=\"https://cdn.shopify.com/s/files/1/1200/7374/products/book_aec28e76-52ec-44ab-bc01-41df1279c89f_550x825.png?v=1473897430\" width=25 align=\"left\"> <font color=\"darkorange\"> &nbsp; Practice II </font><font color=\"skyblue\"> * </font></h2></a>\n",
    "\n",
    "## Loans Description \n",
    "\n",
    "Open the `loans-descs-1k.csv` file as a DataFrame called `loans_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File loans-descs-1k.csv does not exist: 'loans-descs-1k.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0bfc5c11c508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloans_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loans-descs-1k.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mloans_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\User\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\User\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\User\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\User\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\User\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File loans-descs-1k.csv does not exist: 'loans-descs-1k.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "loans_df = pd.read_csv(\"loans-descs-1k.csv\")\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cleaning function\n",
    "\n",
    "Create a cleaning function called `clean()` that cleans the text by applying the following transformations to the `desc` column\n",
    "\n",
    "* 3 regexes with the patterns (do it 1 at a time)\n",
    "\n",
    "```\n",
    "Borrower added on \\d+/\\d+/\\d+ >|<br>\n",
    "<[a-z]+/?>\n",
    "[-_,$&!.;%]\n",
    "```\n",
    "\n",
    "* Strip any extra spaces\n",
    "* Lowercases the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean loans table here\n",
    "def clean():\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the cleaning function\n",
    "\n",
    "Using the `clean()` function, apply it to the `desc` column of the DataFrame, then <b>reassigning it back to the same column.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise 1 Description\n",
    "\n",
    "Randomly select the **first** description and apply:\n",
    "\n",
    "1. Tokenization\n",
    "2. Stemming\n",
    "3. Stopword Removal\n",
    "\n",
    "**Expected output:**\n",
    "\n",
    "    ['wed', 'dream', 'fianceacut', 'plan', 'familyfocus', 'wed', 'hometown', 'us', 'larg', 'famili', 'parent', 'retir', \"'re\", 'cover', 'expens', 'wed', 'prove', 'expens', 'anticip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> End of Practice II </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;10.</font><font color=\"salmon\"> Sentiment Analysis </font> </h2></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10.1'><h3>10.1 VADER</h3></a>\n",
    "\n",
    "The `nltk` library has a sentiment analyser. It uses the VADER method or **Valence Aware Dictionary for\n",
    "sEntiment Reasoning**. It is a lexicon (vocabulary) of words and their relative sentiment strength. For example:\n",
    "    \n",
    "- `Good` has a positive but weak score, while `Excellent` scores more\n",
    "- `Bad` has a negative but weaks score, while `Tragedy` scores more\n",
    "\n",
    "Use `sid.polarity_scores(t)` to find the sentiment of a text. \n",
    "\n",
    "Use the `compound` value to determine the overall score. Note that `compound` give a (normalised) value from $-1$ to $1$, and hence a positive number is good sentiment while a negative number is bad sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the sentiment scores change based on the sentiment of a movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = \"\"\"I thoroughly enjoyed this movie because there was a genuine sincerity in the acting.\"\"\"\n",
    "ss = sid.polarity_scores(review_1)\n",
    "print(ss)\n",
    "print(ss['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it yourself with the following 2 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_2 = \"I found it really boring and silly.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_3 = \"My personal favorite horror film.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10.2'><h3>10.2 Naive Bayes Classifier</h3></a>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Naive Bayes Classifier will be covered in the next lesson on Machine Learning. \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
